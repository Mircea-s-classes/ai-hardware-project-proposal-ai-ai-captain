# AI Detector Project Report

## Project Information
- **Project Name:** Ai Ai Captain â€“ MonkAI Detector
- **Authors:** Argie Cunanan, James Jiang
- **Affiliation / Course:** AI Hardware
- **Date:** December 15, 2025
- **GitHub Repository:** [Link to Repo](https://github.com/Mircea-s-classes/ai-hardware-project-proposal-ai-ai-captain)

---

## Abstract
This project focuses on detecting and classifying hand poses using the OpenMV Cam H7 with lightweight machine learning techniques. The system uses a convolutional neural network trained on approximately 2,000 labeled images (80% training, 20% testing) and runs inference directly on the embedded device. The project demonstrates real-time gesture recognition with visual feedback and minimal latency, highlighting the feasibility of edge-based AI deployment.

---

## 1. Introduction
Embedded machine learning enables real-time perception on low-power hardware. This project explores the use of an embedded vision system to recognize and classify hand gestures using a neural network deployed on-device. By leveraging **Edge Impulse** and the **OpenMV Cam H7**, the MonkAI Detector performs gesture recognition without cloud dependency. Challenges addressed include limited memory, lighting variations, and dataset quality.

---

## 2. Project Goals
The primary goals of this project are:
- Deploy a lightweight neural network-based gesture classifier on embedded hardware using the OpenMV Cam H7
- Recognize and classify **three distinct hand gestures** in real time: finger, mouth, neutral, and thumbs up
- Perform **on-device (edge) inference** with minimal latency
- Provide **visual feedback** confirming the detected gesture

---

## 3. Methodology
The project was developed using **Edge Impulse** and the **OpenMV Cam H7**. Key steps included:
1. Creating an Edge Impulse account and initializing a new project
2. Capturing and labeling images of different hand poses
3. Uploading images to Edge Impulse and splitting data into training and testing sets
4. Training a convolutional neural network (CNN) and optimizing it for embedded deployment
5. Exporting the trained model as a float32 TFLite file
6. Deploying the model to the OpenMV Cam H7 for real-time inference

### 3.1 System Design
- **Image Acquisition Module:** OpenMV camera captures hand gestures  
- **Feature Extraction & Classification:** Pipeline generated by Edge Impulse  
- **Output:** Gesture label displayed through visual indicators on the device  

Captured images are processed locally, and the trained model outputs the predicted gesture class.

### 3.2 Implementation Details
- **Programming language:** Python (MicroPython)  
- **Libraries / frameworks:** OpenMV firmware, Edge Impulse SDK  
- **Detection technique:** CNN for image classification  
- **Input format:** 96x96 grayscale images  
- **Output format:** Gesture label with confidence score  
- **Model architecture:** CNN with three 2D pooling layers  
- **Training parameters:** 15 training cycles, dropout rate 0.1  
- **Model format:** TFLite (float32)  

---

## 4. Procedure
1. Captured ~2,000 images of three hand gestures using the OpenMV Cam H7  
2. Labeled and uploaded images to Edge Impulse  
3. Split data into **training (80%)** and **testing (20%)** sets  
4. Created an impulse with **image input** (96x96) and **classification layer**  
5. Converted images to grayscale to reduce storage and simplify classification  
6. Trained a CNN with three 2D pooling layers for 15 cycles (dropout = 0.1)  
7. Achieved 100% testing accuracy (collected in tandem with training data)  
8. Exported model as float32 TFLite  
9. Deployed model to OpenMV Cam H7  
10. Performed real-time testing and observed classification results  

---

## 5. Results
The MonkAI Detector achieved 100% classification accuracy on the testing dataset, successfully identifying gestures both in local browser testing and on the OpenMV Cam H7 board. The system demonstrated minimal latency after initial classification, although a slight delay was observed when switching between gestures. Converting images to grayscale effectively reduced storage requirements while maintaining classification accuracy. All results were also output via serial to the local machine for verification and logging.

---

## 6. Discussion
These results demonstrate that embedded gesture recognition is feasible using lightweight neural networks. The system performed reliably for clearly defined gestures, but some limitations were noted. Latency between gesture changes, particularly when returning to a neutral position, could affect responsiveness, and the 100% testing accuracy may not fully generalize to unseen data since the test set was collected alongside the training set. Future improvements could include collecting a more diverse dataset, optimizing the system to reduce latency, and testing under varied lighting conditions and gesture sequences to improve robustness.


---

## 7. Conclusion
The MonkAI Detector demonstrates successful deployment of a real-time gesture recognition system on embedded hardware. It highlights both strengths and limitations of edge AI, providing a foundation for future improvements such as adding gestures, improving robustness, and reducing latency.

---

## 8. Usage Instructions
To run or use the project:

```bash
# Install dependencies
pip install -r requirements.txt

# Run the main script
python main.py
